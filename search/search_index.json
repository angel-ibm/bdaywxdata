{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"watsonx.data Partner Badge Days","text":"<p>Herzlich Willkommen zu unserem watsonx.data BadgeDays Workshop</p> <p>Zur Vorbereitung auf unseren Workshop am 5.Dez.2024 bitten wir euch, die folgenden Schritte im Voraus zu erledigen, um einen reibungslosen und produktiven Ablauf sicherzustellen:</p>"},{"location":"#vorbereitung-vor-dem-kurs","title":"Vorbereitung vor dem Kurs","text":"<ul> <li>Link zum L3-Badge-Kurs: IBM watsonx.data for Technical Sales Level 3</li> </ul> <p>Bitte schaut euch vor dem Workshop die folgenden Videos vom Kurs an (siehe Screenshot):</p> <ul> <li>3-Minuten-Video (1. Introduction Video)</li> <li>11-Minuten-Video (3. Enablement Video)</li> </ul> <p></p>"},{"location":"#zugangsvoraussetzungen","title":"Zugangsvoraussetzungen","text":"<p>IBMid erforderlich:  Als Partner ben\u00f6tigt Ihr einen g\u00fcltigen IBMid, um Zugriff auf Seismic und die Techzone zu erhalten. Dieser Zugriff ist notwendig, um die Techzone-Images f\u00fcr das Level 3 Training zu nutzen.</p>"},{"location":"#besondere-voraussetzungen","title":"Besondere Voraussetzungen","text":"<p>SSH/Terminal Setup</p> <ul> <li>F\u00fcr Windows: Bitte installiert Putty f\u00fcr den SSH-Zugriff.</li> <li>F\u00fcr MacOS: Stellt sicher, dass das Terminal-Programm einsatzbereit ist.</li> </ul> <p>Internetverbindung</p> <p>Stellt sicher, dass euer Netzwerkzugang zu folgenden Domains m\u00f6glich ist:</p> <ul> <li><code>*.ibm.com</code></li> <li><code>*.box.com</code></li> </ul>"},{"location":"#ablauf-des-workshops","title":"Ablauf des Workshops","text":"<ul> <li>Kurze Einf\u00fchrung (~30 Minuten): Wir werden  eine Einf\u00fchrung in watsonx.data geben, was es ist und wof\u00fcr es verwendet wird (Use Cases).</li> <li>Live-Demo (~10 Minuten): Anschlie\u00dfend zeige ich in einer Live-Demo, wie man sich in der UI von watsonx.ai zurechfindet.</li> <li>Version 2.0.2: Wir verwenden Version 2.0.2, da diese zur Level 3 Anleitung passt, obwohl bereits eine neuere Version verf\u00fcgbar ist.</li> <li>Eigenst\u00e4ndiges Arbeiten (~1 Stunde): Danach folgt ihr der Lab-Anleitung selbstst\u00e4ndig (meist Copy &amp; Paste) und lest/versteht die Inhalte. IBM</li> <li>Offene Diskussion (~30-60 Minuten): Wir haben dann eine offene Diskussion, falls noch Fragen bestehen.</li> <li>Badge (~10 Minuten): Abschlie\u00dfend k\u00f6nnt ihr euch den Badge verdienen: Level 3 Badge. (5 Fragen)</li> </ul>"},{"location":"#wichtige-links","title":"Wichtige Links","text":"<ul> <li>Techzone Image: Link zur Techzone Developer Image</li> <li>Dokumentation f\u00fcr das IBM watsonx.data Development Lab (Version 2.0.2 GA) f\u00fcr die Level 3 Zertifizierung: Link zur Dokumentation</li> <li>IBM Seismic: Link zur Seismic</li> <li>IBM Techzone (w3id): Link zur Techzone</li> </ul>"},{"location":"#prasentation","title":"Pr\u00e4sentation","text":"<p>Das ist ein tieferer Einblick in watsonx.data</p> <p>Pr\u00e4sentation - PDF Datei </p> <p>Vorschau der Pr\u00e4sentation:</p>"},{"location":"alex/","title":"Command Guide","text":"<p>Die folgenden Befehle erleichtert die Durchf\u00fchrung von den \u00dcbungen. Die Abschnitt- und Seitennummer beziehen sich auf den Sektionen der Dokumentation von watsonx.data Lab  </p>"},{"location":"alex/#42-accessing-the-watsonxdata-environment","title":"4.2. Accessing the watsonx.data Environment","text":"<p>Page | 13 <pre><code>ssh -p 99999 watsonx@geo.services.cloud.techzone.ibm.com\n</code></pre> password: <code>watsonx.data</code></p> <p>This command is used to switch to the root user with superuser privileges. Page | 16 <pre><code>sudo su -\n</code></pre></p>"},{"location":"alex/#451-stopping-watsonxdata","title":"4.5.1   Stopping watsonx.data","text":"<p>Page | 20 <pre><code>sudo su -\ncd /root/ibm-lh-dev/bin\n</code></pre> <pre><code>./stop\n./status --all\n</code></pre></p>"},{"location":"alex/#452-starting-watsonxdata","title":"4.5.2. Starting watsonx.data","text":"<p>Page | 21 <pre><code>export LH_RUN_MODE=diag\n./start\n./status --all\n</code></pre></p>"},{"location":"alex/#516-starting-the-watsonxdata-user-interface","title":"5.1.6 Starting the watsonx.data User Interface","text":"<p>Page | 25 Username: <pre><code>ibmlhadmin\n</code></pre> Password: <pre><code>password\n</code></pre></p>"},{"location":"alex/#5311-download-the-sample-carscsv-file-to-your-desktop","title":"5.3.11 Download the sample cars.csv file to your desktop","text":"<p>The Create table from file workflow allows you to upload a small (maximum 2 MB file size) .csv, .parquet, .json, or .txt file to define and populate a new table. Download cars.csv </p> <p>Catalog: iceberg_data Schema: my_schema Table name: cars Table format: Apache Iceberg Data format: Parquet Data format version: Parquet v1</p>"},{"location":"alex/#542-query-workspace-page","title":"5.4.2 Query Workspace Page","text":"<p>Copy and paste the following text into the SQL worksheet. Note that the table you are about to query is identified by a 3-part name that includes the catalog, schema, and table name. Click Run on presto-01. Page | 46 <pre><code>select car, avg(mpg) as avg_mpg from iceberg_data.my_schema.cars group by car order by car;\n</code></pre></p>"},{"location":"alex/#562-access-control-page","title":"5.6.2 Access Control Page","text":"<p>Open a terminal command window to the watsonx.data server as the root user (remember to use the sudo command to become the root user Page | 56 <pre><code>/root/ibm-lh-dev/bin/user-mgmt add-user user1 User password1\n</code></pre></p>"},{"location":"alex/#612-presto-command-line-interface-cli","title":"6.1.2 Presto Command Line Interface (CLI)","text":"<p>Page | 64 <pre><code>cd /root/ibm-lh-dev/bin\n./presto-cli\n</code></pre> Page | 65 <pre><code>## Show the catalogs available in the system.\nshow catalogs;\n</code></pre> Page | 65 <pre><code>## Query the customer table in the tpch catalog.\nselect count(*) from tpch.tiny.customer;\n</code></pre> Page | 65 <pre><code>## Show the schemas available in the tpch catalog.\nshow schemas in tpch;\n</code></pre> Page | 66 <pre><code>## Use the tpch.tiny schema.\nuse tpch.tiny;\n</code></pre> Page | 66 <pre><code>## Query the customer table in the tpch.tiny schema.\nselect count(*) from customer;\n</code></pre> Page | 66 <pre><code>quit;\n</code></pre> Page | 67 <pre><code>./presto-cli --catalog tpch --schema tiny\n</code></pre> Page | 67 <pre><code>select count(*) from customer;\n</code></pre> Page | 67 <pre><code>quit;\n</code></pre> Page | 67 <pre><code>./presto-cli --catalog iceberg_data\n</code></pre> Page | 67 <pre><code>create schema if not exists newschema with (location='s3a://iceberg-bucket/newschema');\n</code></pre> Page | 67 <pre><code>show schemas;\n</code></pre> Page | 68 <pre><code>create table newschema.users (id int, name varchar, age int);\n</code></pre> Page | 68 <pre><code>insert into newschema.users values (1, 'Robert', 54);\ninsert into newschema.users values (2, 'Susan', 37);\nselect * from newschema.users order by id;\n</code></pre> Page | 68 <pre><code>use newschema;\nshow tables;\nquit;\n</code></pre></p>"},{"location":"alex/#72-exploring-minio-object-storage","title":"7.2. Exploring MinIO Object Storage","text":"<p>Page | 75 <pre><code>docker exec ibm-lh-presto printenv | grep LH_S3_ACCESS_KEY | sed's/.*=//'\n</code></pre> Page | 75 <pre><code>docker exec ibm-lh-presto printenv | grep LH_S3_SECRET_KEY | sed 's/.*=//'\n</code></pre></p>"},{"location":"alex/#88-data-ingestion","title":"8.8 Data Ingestion","text":"<p>Page | 79 You can download the <code>aircraft.parquet</code> file from the following URL:</p> <p>Download aircraft.parquet </p> <p>Page | 82 <pre><code>/root/ibm-lh-dev/bin/presto-cli\n</code></pre> Page | 82 <pre><code>create schema hive_data.myschema2 with (location = 's3a://hive-bucket/myschema2');\n</code></pre> Page | 82 <pre><code>create table hive_data.myschema2.aircraft\n(tail_number varchar,\nmanufacturer varchar,\nmodel varchar)\nwith (format = 'Parquet',\nexternal_location='s3a://hive-bucket/myschema2/aircraft');\n</code></pre> Page | 82 <pre><code>select count(*) from hive_data.myschema2.aircraft;\nquit;\n</code></pre></p>"},{"location":"alex/#94-federated-queries","title":"9.4 Federated Queries","text":"<p>Page | 86</p> <ul> <li>Display name: <code>Db2DW</code></li> <li>Database name: <code>GOSALES</code></li> <li>Hostname: <code>192.168.252.2</code></li> <li>Port: <code>50000</code></li> <li>Authentication type: <code>Username and password</code></li> <li>Username: <code>db2inst1</code></li> <li>Password: <code>db2inst1</code></li> <li>Port is SSL enabled: <code>&lt;toggled off&gt;</code></li> </ul> <p>Page | 90 <pre><code>docker exec ibm-lh-postgres printenv | grep POSTGRES_PASSWORD | sed 's/.*=//'\n</code></pre></p> <ul> <li>Display name: <code>PostgreSQLDB</code></li> <li>Database name: <code>gosales</code></li> <li>Hostname: <code>192.168.252.2</code></li> <li>Port: <code>5432</code></li> <li>Username: <code>admin</code></li> <li>Password: <code>&lt;The password generated in the previous step&gt;</code></li> <li>Port is SSL enabled: <code>&lt;toggled off&gt;</code></li> <li>Catalog name: <code>pgcatalog</code></li> </ul> <p>Page | 93 <pre><code>select pll.product_line_en as product,\nmd.order_method_en as order_method,\nsum(sf.quantity) as total\nfrom\npgcatalog.gosalesdw.sls_order_method_dim as md,\ndb2catalog.gosalesdw.sls_product_dim as pd,\ndb2catalog.gosalesdw.sls\nproduct\n_\n_line_lookup as pll,\nhive_data.gosalesdw.sls_product_brand_lookup as pbl,\nhive_data.gosalesdw.sls_sales_fact as sf\nwhere\npd.product_key = sf.product_key\nand md.order_method_key = sf.order_method_key\nand pll.product_line_code = pd.product_line_code\nand pbl.product_brand_code = pd.product_brand_code\ngroup by pll.product_line_en, md.order_method_en\norder by product, order_method;\n</code></pre></p> <p>Page | 95 <pre><code>select distinct branch_region_dim.region_en region,\nbranch_region_dim.country_en country,\nemp_employee_dim.employee_name employee\nfrom hive_data.gosalesdw.go_region_dim branch_region_dim,\npgcatalog.gosalesdw.emp_employee_dim emp_employee_dim,\ndb2catalog.gosalesdw.go_branch_dim go_branch_dim\nwhere branch_region_dim.country_en in ('Canada', 'Mexico') and\nbranch_region_dim.country_code = go_branch_dim.country_code and\nemp_employee_dim.branch_code = go_branch_dim.branch_code\norder by region, country, employee;\n</code></pre></p> <p>Page | 96 <pre><code>select gosalesdw.go_org_dim.organization_key,\ngo_org_dim_1.organization_parent as org_level1_code,\ngo_org_name_lookup_1.organization_name_en as org_level1_name,\ngosalesdw.go_org_dim.organization_parent as org_level2_code,\ngo_org_name_lookup_2.organization_name_en as org_level2_name,\ngosalesdw.go_org_dim.organization_code as org_code,\ngosalesdw.go_org_name_lookup.organization_name_en as org_name\nfrom db2catalog.gosalesdw.go_org_name_lookup go_org_name_lookup_2\ninner join\nhive_data.gosalesdw.go_org_dim\ninner join\npgcatalog.gosalesdw.go_org_name_lookup\non hive_data.gosalesdw.go_org_dim.organization_code =\npgcatalog.gosalesdw.go_org_name_lookup.organization_code\non go_org_name_lookup_2.organization_code =\nhive_data.gosalesdw.go_org_dim.organization_parent\ninner join\npgcatalog.gosalesdw.go_org_name_lookup go_org_name_lookup_1\ninner join\nhive_data.gosalesdw.go_org_dim go_org_dim_1\non go_org_name_lookup_1.organization_code =\ngo_org_dim_1.organization_parent\non hive_data.gosalesdw.go_org_dim.organization_parent =\ngo_org_dim_1.organization_code\nwhere (hive_data.gosalesdw.go_org_dim.organization_code\nbetween '1700' and '5730')\norder by org_code;\n</code></pre></p>"},{"location":"alex/#10-offloading-data-from-a-data-warehouse","title":"10 Offloading Data from a Data Warehouse","text":"<p>Page | 98 <pre><code>iceberg_data\n</code></pre> <pre><code>wxgosalesdw\n</code></pre></p>"},{"location":"alex/#106-sql","title":"10.6 SQL","text":"<p>Page | 99 <pre><code>create table iceberg_data.wxgosalesdw.sls_sales_fact as select * from db2catalog.gosalesdw.sls_sales_fact;\n</code></pre></p>"},{"location":"alex/#107-federated-queries","title":"10.7 Federated Queries","text":"<p>Page | 100 <pre><code>select pll.product_line_en as product,\nsum(sf.quantity) as total\nfrom\ndb2catalog.gosalesdw.sls_product_dim as pd,\ndb2catalog.gosalesdw.sls_product_line_lookup as pll,\niceberg_data.wxgosalesdw.sls_sales_fact as sf\nwhere\npd.product_key = sf.product_key\nand pll.product_line_code = pd.product_line_code\ngroup by pll.product_line_en\norder by product;\n</code></pre></p>"},{"location":"alex/#1112-table-rollback","title":"11.1.2 Table Rollback","text":"<p>Page | 101 <pre><code>create table iceberg_data.my_schema.airport_id\nas select * from hive_data.ontime.airport_id;\n</code></pre></p> <p>Page | 102 <pre><code>insert into iceberg_data.my_schema.airport_id values (10000, 'North Pole: Reindeer Field');\nselect * from iceberg_data.my_schema.airport_id where code = 10000;\nselect count(*) from iceberg_data.my_schema.airport_id;\n</code></pre></p> <p>Page | 104 <pre><code>call iceberg_data.system.rollback_to_snapshot ('my_schema', 'airport_id', &lt;snapshotID&gt;);\n</code></pre></p> <p>Page | 105 <pre><code>select * from iceberg_data.my_schema.airport_id where code = 10000;\nselect count(*) from iceberg_data.my_schema.airport_id;\n</code></pre></p>"},{"location":"alex/#1122-time-travel-queries","title":"11.2.2 Time Travel Queries","text":"<p>Page | 105 <pre><code>create table iceberg_data.my_schema.ttqtable (id int, name char(10));\n</code></pre> <pre><code>select * from iceberg_data.my_schema.\"ttqtable$snapshots\" order by committed_at;\n</code></pre> Page | 107 <pre><code>insert into iceberg_data.my_schema.ttqtable values (1, 'TV'), (2, 'Computer'), (3, 'Stereo');\n</code></pre> <pre><code>select * from iceberg_data.my_schema.\"ttqtable$snapshots\" order by committed_at;\n</code></pre> Page | 108 <pre><code>insert into iceberg_data.my_schema.ttqtable values (4, 'Phone'), (5, 'Tablet'), (6, 'Camera');\ninsert into iceberg_data.my_schema.ttqtable values (7, 'Headphones'), (8, 'Microphone'), (9, 'Watch');\n</code></pre> <pre><code>select * from iceberg_data.my_schema.\"ttqtable$snapshots\" order by committed_at;\n</code></pre></p>"},{"location":"alex/#11210-time-travel-queries","title":"11.2.10 Time Travel Queries","text":"<p>Page | 109 <pre><code>select * from iceberg_data.my_schema.ttqtable for version as of 'Transaction1SnapshotID' order by id;\n</code></pre> <pre><code>select * from iceberg_data.my_schema.ttqtable for timestamp as of cast('Tran1Time' as timestamp with time zone) order by id;\n</code></pre> <pre><code>select * from iceberg_data.my_schema.ttqtable for version as of Tran2SnapshotID order by id;\n</code></pre> <pre><code>select * from iceberg_data.my_schema.ttqtable for timestamp as of cast('Tran3Time' as timestamp with time zone) order by id;\n</code></pre> Page | 110 <pre><code>select * from iceberg_data.my_schema.ttqtable for timestamp as of cast('Pick-a-time-between-Tran2Time-and-Tran3Time' as timestamp with time zone) order by id;\n</code></pre> <pre><code>select * from iceberg_data.my_schema.ttqtable for timestamp as of cast('2023-01-01 00:00:00.000 UTC' as timestamp with time zone) order by id;\n</code></pre></p>"}]}